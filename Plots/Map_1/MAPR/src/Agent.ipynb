{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1c4fe-47b9-41a0-9893-13f7f7b06ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/utils.ipynb\n",
    "%run src/DQN/dqn_agent.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6abf29-a14f-470b-8754-369a7b2a0a90",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff92a154-b95d-43a1-889f-29878346ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math                  \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc66deac-cc76-4063-b090-5e66b6c693e0",
   "metadata": {},
   "source": [
    "read the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f9f74-d4a7-454e-adfe-181ee1b9a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "  \n",
    "configur = ConfigParser()\n",
    "import builtins\n",
    "configur.read(builtins.current_filename)\n",
    "\n",
    "#configur.read('config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338bf2ed-4188-4ec2-9c44-46a2587bfd95",
   "metadata": {},
   "source": [
    "variable names read from the configuration file.\n",
    "- maxTtl - don't know\n",
    "- defaultTtl - def Ttl for each packet\n",
    "- packet_drop_reward - penalty for dropping a packet\n",
    "- ttl_zero_reward - reward if the packet expires\n",
    "- agent_to_agent_scale = scaling of the reward during transfer of the packet from agent to another\n",
    "- scaling_type - the type of scaling used to return the reward.\n",
    "- include_distance - should the manhattan distance of the receiving agent be considered in the reward for the receiving agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab513e1c-1ac4-4abd-8ab2-4ccb7d86f7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTtl = int(configur.get('packet','maxTtl')) \n",
    "defaultTtl = int(configur.get('packet','def_ttl')) \n",
    "packet_drop_reward = int(configur.get('reward','packet_drop_reward'))\n",
    "ttl_zero_reward = int(configur.get('reward','ttl_zero_reward'))\n",
    "agent_to_agent_scale = float(configur.get('reward','agent_to_agent_scale'))\n",
    "scaling_type = configur.get('scaling_factor','type')\n",
    "include_distance = configur.getboolean('reward','include_distance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa51b3-b1d9-42c1-92aa-5e038f8e242c",
   "metadata": {},
   "source": [
    "The agent is basically a UAV node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70357c5-fdb6-4c65-a77c-3a286142bd79",
   "metadata": {},
   "source": [
    "The Agent is initialised by the following parameters - \n",
    "- queue - list of all the packets\n",
    "- neighbours - list of all the neighbours probabily\n",
    "- dqn_object - DQN or rather DDQN for the agent\n",
    "- position - position of the agent in the grid\n",
    "- batchsize - batchsize used to train the agent\n",
    "- state_size - size of states\n",
    "- action_size - number of possible actions\n",
    "- targetBaseStation - The base station to which the packet should be delivered\n",
    "- latest_loss - most recent loss computed from training the policy DQN of the agent\n",
    "- losses - list of all the losses of the agent.\n",
    "- latest_queue - the queue after each network run\n",
    "- q_values - the permanent queue which is updated after each network run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6cd717-bd60-4357-b641-e867e2356682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class agent\n",
    "class Agent():\n",
    "    def __init__(self,neighbours, x,y,BaseStation, batchsize = 64):    \n",
    "        self.queue = []\n",
    "        self.neighbours = neighbours\n",
    "        self.dqn_object = None\n",
    "        self.position = (x,y)\n",
    "        self.batchsize = batchsize\n",
    "        self.state_size = 1  ## TODO : Removed head packet ttl from state\n",
    "        self.action_size = 1\n",
    "        self.targetBaseStation = BaseStation \n",
    "        self.latest_loss = 0\n",
    "        self.losses = []\n",
    "        self.latest_queue = []\n",
    "        self.q_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f913422-5a32-4f0c-878c-e8bf871e0a32",
   "metadata": {},
   "source": [
    "getCurrentState\n",
    "- state - is equal to the size of its queue, and th queue size of all the neighbours\n",
    "- the base node has a queue length of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45c9e3-136b-4693-bb7e-c34062867ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def getCurrentState(self):\n",
    "        \"\"\"\n",
    "            current queue size, queue sizes of all neighbours, remaining ttl of packet at head\n",
    "        \"\"\"\n",
    "        state = [len(self.queue)]\n",
    "        for neighbour in self.neighbours:\n",
    "            if not neighbour.isBase():\n",
    "                state.append(len(neighbour.queue))\n",
    "            else:\n",
    "                state.append(0)   # base station is always given empty queue, denoting availability TODO: think\n",
    "        # if len(self.queue)>0:\n",
    "        #     state.append(self.queue[0].get_ttl())\n",
    "        # else:\n",
    "        #     state.append(maxTtl) #: MAX_TTL??\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b382c-b4d2-411d-b746-fd945c5a6aea",
   "metadata": {},
   "source": [
    "initDQN - DQNAgent - DDQN for the agent \\\n",
    "loadModel - load the DQNAgent \\\n",
    "pushQueue - append a packet to the agent's queue \\\n",
    "popQueue - remove the packet at the head of the queue else return -1. \\\n",
    "getTopPacket - obtain the packet at the head of the queue else return -1. \\\n",
    "nextAction - select the action using the policy DQN in an epsilon greedy manner. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5360c8-0fc8-44c3-a2b2-36cdfda72dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def initDQN(self,device):\n",
    "        self.dqn_object = DQNAgent(device ,self.state_size, self.action_size)        #  add parameters here\n",
    "\n",
    "    def loadModel(self,filename):\n",
    "        self.dqn_object.loadModel(filename)\n",
    "\n",
    "    def pushQueue(self, packet):\n",
    "        self.queue.append(packet)\n",
    "        return True                            ## return true if packet is pushed for ensuring queue is not full\n",
    "    \n",
    "    def popQueue(self):\n",
    "        if len(self.queue) == 0 :\n",
    "            return -1\n",
    "        return self.queue.pop(0)\n",
    "\n",
    "    def getTopPacket(self):\n",
    "        if len(self.latest_queue) == 0:\n",
    "            return -1\n",
    "        return self.latest_queue[-1]\n",
    "\n",
    "    def nextAction(self,state):\n",
    "        # use dqn to find this\n",
    "        return self.dqn_object.selectAction(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c6cd8-a79c-460f-8005-463be74b180f",
   "metadata": {},
   "source": [
    "trainAgent\n",
    "- store the experience of a single state, action, next_state, and reward.\n",
    "- then train the policy model over an entire batch and update its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31092373-78cc-4a2e-9d62-3d99299e496c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def trainAgent(self,state,action,nextState,reward):\n",
    "        # use dqn to train this\n",
    "\n",
    "        self.dqn_object.memory.store(state=state, action=action, next_state=nextState, reward=reward)\n",
    "        self.latest_loss = self.dqn_object.learn(batchsize=self.batchsize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127f225c-cf93-4c06-a2dd-c5a0e3c5b922",
   "metadata": {},
   "source": [
    "saveLoss\n",
    "- append latest loss to the losses list\n",
    "getLoss\n",
    "- return the entire losses list\n",
    "acceptPacket\n",
    "- append the packet to the latest queue. Don't know what is the difference b/w queue and latest queue.\n",
    "getPosition\n",
    "- return agent's position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54abc96-847d-4727-9b43-18caf4dbce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def saveLoss(self):\n",
    "        self.losses.append(self.latest_loss)\n",
    "\n",
    "    def getLoss(self):  \n",
    "        return self.losses\n",
    "\n",
    "    def acceptPacket(self,packet):\n",
    "        ## TODO add queue size \n",
    "        self.latest_queue.append(packet)\n",
    "\n",
    "    def getPosition(self):\n",
    "        return self.position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772883d-5ddf-4104-8364-fa4370fef303",
   "metadata": {},
   "source": [
    "addNeighbour\n",
    "- add the neighbour to the neighbours list\n",
    "- increase the action size and state size by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a7287-30c6-4357-a73f-05be097d86a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def addNeighbour(self,neighbour):\n",
    "        self.neighbours.append(neighbour)\n",
    "        self.action_size+=1\n",
    "        self.state_size+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456b2e3-9d66-4dfe-87ab-fda9665c1e8e",
   "metadata": {},
   "source": [
    "isUAV - True \\\n",
    "isBase, isBaseStation, isIot, isBlock - False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54cbca-b149-4130-8868-b3195319b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def isUAV(self):\n",
    "        return True\n",
    "    \n",
    "    def isBase(self):\n",
    "        return False\n",
    "     \n",
    "    def isIot(self):\n",
    "        return False\n",
    "\n",
    "    def isBaseStation(self):\n",
    "        return False\n",
    "\n",
    "    def isBlock(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e48bc-9a6b-4eb8-8ffc-6b1133a1c4c7",
   "metadata": {},
   "source": [
    "getReward\n",
    "- get the ttl for the top packet.\n",
    "    - self.getTopPacket().get_ttl() --> head_packet.get_ttl() --> def_ttl for top packet \n",
    "- get the reward\n",
    "    - agent_to_agent_scale*self.dqn_object.getQValue(self.getCurrentState())\n",
    "        - agent_to_agent_scale*DQNAgent * TargetDQN_Q_value(state = \\[its queue size and its neighbours queue size\\])\n",
    "- scale the reward by the packets' own ttl which will be less and the default ttl of each new package\n",
    "- return the scaled reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78db46-c218-4594-b83b-755ef270013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def getReward(self):       \n",
    "        top_packet_ttl = self.getTopPacket().get_ttl()    \n",
    "        reward = agent_to_agent_scale*self.dqn_object.getQValue(self.getCurrentState())    # TODO Scale this value. \n",
    "        if scaling_type == 'square':\n",
    "            scaled_reward = reward*((top_packet_ttl/defaultTtl)**2)\n",
    "        elif scaling_type == 'exponential':\n",
    "            scaled_reward = reward*math.exp(top_packet_ttl-defaultTtl)     \n",
    "        elif scaling_type == 'fraction': \n",
    "            scaled_reward = reward*(top_packet_ttl/defaultTtl)\n",
    "        else:\n",
    "            scaled_reward = reward\n",
    "        # print(\"postion {},reward {}\".format(self.getPosition(), scaled_reward))         # based on q-value \n",
    "        return scaled_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e4d42d-800d-4b90-841a-8a253c26d920",
   "metadata": {},
   "source": [
    "ttl - decrease which each run and not with each hop\n",
    "\n",
    "- obtain the current state\n",
    "- decrease the ttl for all the packets with the agent.\n",
    "- remove the top packet from the queue, and store it is *topPacket*.\n",
    "- if the queue is empty there is nothing to do\n",
    "- then select the action from the policy network using epsilon greedy method\n",
    "- obtain q_value from the targetDQN and append it to the q_values list.\n",
    "- if the agent is not in training mode:\n",
    "    - print its position, state, top packets ttl,\n",
    "    - apparently there seems to be an action for dropping the packet - Doubt - Pending\n",
    "    - else print the neighbour who receives the packet, and send the packet to that neighbour, and display its position\n",
    "    - print the current q_values\n",
    "    - plot the history of q_values and save thee figure\n",
    "    - what about the history of losses\n",
    "\n",
    "- nextState = get the state\n",
    "- it the *topPacket* expired or got dropped, then it can't be sent to the neighbours, so train the agent, if train option is selected. (the experience gets stored, and param gets updated)\n",
    "    - ttl_zero_reward and packet_drop_reward are variables with negative values\n",
    "- if the above conditions do not hold, then the packet can safely be sent to the neighbour, which appends it in its list, which is basically the action being performed\n",
    "- Then after the action is performed, we obtain the *nextState*\n",
    "- Doubt-Pending, why is there a latest_queue and queue, is this something to do with the queue length.\n",
    "- as the latest_queue only gets updated, and the queue is left as it is, but doesn't the gap eventually add up.\n",
    "- so, the next_State, next_Action+1, needs to be increased by +1.\n",
    "- then we obtain the local reward, which is scaled by the ManhattanDistance between the receiving agent(in the paper) and current agent and the destination.\n",
    "- if train option is selected,\n",
    "    - train the agent \n",
    "\n",
    "- and how is the return value computed - especially the global reward - Doubt - Pending - FIND OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a5a42-d735-465f-b8ca-3c111dae9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def run(self, train = True):\n",
    "        state = self.getCurrentState()\n",
    "        for packet in self.queue:  \n",
    "            packet.decrease_ttl()         # ttl of packet decreases\n",
    "        topPacket = self.popQueue()\n",
    "\n",
    "        if topPacket == -1:\n",
    "            #print('empty top packet')\n",
    "            reward = 0\n",
    "            return reward # if the queue is already empty, nothing to do\n",
    "\n",
    "        \n",
    "        nextAction = self.nextAction(state)                ## from dqn\n",
    "        self.q_values.append(self.dqn_object.getQValue(state))\n",
    "\n",
    "        if not train:\n",
    "            print(\"Position : \",self.getPosition())\n",
    "            print(\"States : \", state)\n",
    "            print(\"TTL : \", topPacket.get_ttl())\n",
    "            if(nextAction == len(self.neighbours)):\n",
    "                print(\"Packet dropped\")\n",
    "            else:\n",
    "                print(\"Packet forwarded to neighbour : \",self.neighbours[nextAction].getPosition())\n",
    "            print(\"Q-Value : \", self.dqn_object.getQValue(state))\n",
    "            plt.plot(self.q_values)\n",
    "            plt.savefig(f'./agent_at_{self.position}.png')\n",
    "            plt.close()\n",
    "\n",
    "        nextState = self.getCurrentState()\n",
    "        if topPacket.get_ttl() <= 0:\n",
    "            #print('packet being dropped')\n",
    "            if train:\n",
    "                self.trainAgent(state,nextAction,nextState,ttl_zero_reward) \n",
    "            return -100\n",
    "        \n",
    "        if nextAction == len(self.neighbours):\n",
    "            nextAction = 0\n",
    "            # print('Not Sent')\n",
    "            # if train:  \n",
    "            #     self.trainAgent(state,nextAction,nextState,packet_drop_reward) \n",
    "            # return -100\n",
    "\n",
    "        #print('packet being sent')\n",
    "        self.neighbours[nextAction].acceptPacket(topPacket)  ## push to next agent\n",
    "        nextState = self.getCurrentState()\n",
    "        nextState[nextAction+1]+=1 #nextState[nextAction+1]+=1\n",
    "        #TODO: reward should be based on q-value and TTL\n",
    "        #reward = getManhattanDistance(self.getPosition(), self.targetBaseStation.getPosition()) + self.neighbours[nextAction].getReward()\n",
    "        reward = self.neighbours[nextAction].getReward()\n",
    "        defTrl = 100\n",
    "        reward_to_be_returned = 100*(topPacket.get_ttl()/defTtl)\n",
    "        if(include_distance):\n",
    "            reward *= 1/getManhattanDistance(self.getPosition(), self.targetBaseStation.getPosition())\n",
    "\n",
    "        man_dist = getManhattanDistance(self.getPosition(), self.targetBaseStation.getPosition())\n",
    "        if man_dist == 0:\n",
    "            reward_to_be_returned += 100\n",
    "        else:\n",
    "            reward_to_be_returned /= man_dist\n",
    "\n",
    "        if train:\n",
    "            self.trainAgent(state,nextAction,nextState,reward) \n",
    "\n",
    "        return reward_to_be_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294123d0-315b-4684-b3f8-2c566da83c40",
   "metadata": {},
   "source": [
    "randomRun\n",
    "same as run\n",
    "- but q_value is not being calculates and stored.\n",
    "- the status if not train , is not being displayed.\n",
    "- it returns nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b172a468-2a8a-4ae2-81ad-6d7c55e3819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):\n",
    "    def randomRun(self):\n",
    "        state = self.getCurrentState()\n",
    "        for packet in self.queue:  \n",
    "            packet.decrease_ttl()         # ttl of packet decreases\n",
    "        topPacket = self.popQueue()\n",
    "        # topPacket.decrease_ttl()         # ttl of packet decreases\n",
    "\n",
    "        if topPacket == -1:\n",
    "            return # if the queue is already empty, nothing to do\n",
    "\n",
    "        action = random.randint(0,len(self.neighbours))\n",
    "        if(topPacket.get_ttl() <= 0):\n",
    "            reward = ttl_zero_reward\n",
    "            nextState = self.getCurrentState()\n",
    "            self.dqn_object.memory.store(state=state, action=action, next_state=nextState, reward=reward)\n",
    "        \n",
    "        if action == len(self.neighbours):\n",
    "            reward = packet_drop_reward\n",
    "            nextState = self.getCurrentState()\n",
    "            self.dqn_object.memory.store(state=state, action=action, next_state=nextState, reward=reward)\n",
    "        \n",
    "        else:    \n",
    "            self.neighbours[action].acceptPacket(topPacket)  ## push to next agent\n",
    "            #TODO: reward should be based on q-value and TTL\n",
    "            \n",
    "            #reward = getManhattanDistance(self.getPosition(), self.targetBaseStation.getPosition()) + self.neighbours[action].getReward()\n",
    "            reward = self.neighbours[action].getReward()\n",
    "            \n",
    "            if(include_distance):\n",
    "                reward *= 1/getManhattanDistance(self.getPosition(), self.targetBaseStation.getPosition())\n",
    "\n",
    "            nextState = self.getCurrentState()\n",
    "            nextState[action+1] += 1 #nextState[action+1]+=1\n",
    "            self.dqn_object.memory.store(state=state, action=action, next_state=nextState, reward=reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29029103-0636-4abe-a31d-780ef4d6d8e6",
   "metadata": {},
   "source": [
    "getVal\n",
    "- returns the length of the queue\n",
    "reset\n",
    "- make queue empty\n",
    "update_state\n",
    "- for packets in latest_queue, append them to the main queue, and make the latest_queue empty.\n",
    "- Still don't see the purpose of maintaining two queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3204b92-4b60-4678-8e99-f3b578456c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Agent):    \n",
    "    def getVal(self):\n",
    "        return len(self.queue) + len(self.latest_queue)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset everything in the agent to turn on test mode\n",
    "        \"\"\"\n",
    "        self.queue = []\n",
    "\n",
    "    def update_state(self):\n",
    "        for packets in self.latest_queue:\n",
    "            self.queue.append(packets)\n",
    "        self.latest_queue = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbe2e63-39f2-4d03-9728-68f49123d155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
