{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f9820d-21c4-4ab9-bdea-234dcd7de20e",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20184db-df07-4f6e-bcf4-13f726c882b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nScript that contains the details about the experience replay buffer used in DDQN to ensure training stability\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script that contains the details about the experience replay buffer used in DDQN to ensure training stability\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abcceb-b921-45ee-9b2d-0df9d8f255b9",
   "metadata": {},
   "source": [
    "Import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e2b249-2507-4bb0-aa60-a05539d422b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb55c9b-0f3f-49fa-8973-5931fdf2d244",
   "metadata": {},
   "source": [
    "Read the variables from the configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "553458e2-4405-4b26-9a1d-e0f300992f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/config.ini']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configparser import ConfigParser\n",
    "  \n",
    "configur = ConfigParser()\n",
    "import builtins\n",
    "configur.read(builtins.current_filename)\n",
    "\n",
    "# configur.read('config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d577e93-4095-44e6-8005-a4ccc834268c",
   "metadata": {},
   "source": [
    "self.capacity is the capacity of the replay buffer.\n",
    "The replay buffer has a circular memory.\n",
    "- self.buffer_state = [] # Stores all the current states\n",
    "- self.buffer_action = [] # Stores all the current actions\n",
    "- self.buffer_next_state = [] # Stores all the next actions\n",
    "- self.buffer_reward = [] # Stores all the rewards\n",
    "- self.idx = 0 # This is the position where the next experience will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b3e10f-a017-4f8a-8739-4f2f04acae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \"\"\"\n",
    "    Class representing the replay buffer used for storing experiences for off-policy learning\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer_state = []\n",
    "        self.buffer_action = []\n",
    "        self.buffer_next_state = []\n",
    "        self.buffer_reward = []\n",
    "        # self.buffer_done = []\n",
    "        self.idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd8e84f-5c63-4156-8eaf-4dd5da08fdd8",
   "metadata": {},
   "source": [
    "Stores the current experience in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab9a1e9-4c85-4bc3-a551-b4627ed954b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(ReplayMemory):\n",
    "    def store(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        Function to add the provided experience to the memory, such that transition is a 5-tuple of the form (state, action, next_state, reward, done)\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        state: numpy.ndarray\n",
    "            Current state vector observed in the environment\n",
    "        action: int\n",
    "            Action performed by the agent in the current state\n",
    "        next_state: numpy.ndarray\n",
    "            State vector observed as a result of performing the action in the current state\n",
    "        reward: float\n",
    "            Reward obtained by the agent\n",
    "        done: bool\n",
    "            Indicates whether the agent has entered a terminal state or not\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        if len(self.buffer_state) < self.capacity:\n",
    "            self.buffer_state.append(state)\n",
    "            self.buffer_action.append(action)\n",
    "            self.buffer_next_state.append(next_state)\n",
    "            self.buffer_reward.append(reward)\n",
    "            # self.buffer_done.append(done)\n",
    "        else:\n",
    "            self.buffer_state[self.idx] = state\n",
    "            self.buffer_action[self.idx] = action\n",
    "            self.buffer_next_state[self.idx] = next_state\n",
    "            self.buffer_reward[self.idx] = reward\n",
    "            # self.buffer_done[self.idx] = done\n",
    "\n",
    "        self.idx = (self.idx+1)%self.capacity # for circular memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921320f-9292-4647-98dc-2aca38e24d4f",
   "metadata": {},
   "source": [
    "Return Randomly choose batchsize number of indices. \\\n",
    "Make the state into a numpy ndarray, and then sample the given indices, convert it back to a tensor.\n",
    "return states, actions, next_states, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1a5d603-6f91-4355-ba36-d38a7fefbe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(ReplayMemory):\n",
    "    def sample(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        Function to pick 'n' samples from the memory that are selected uniformly at random, such that n = batchsize\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        batchsize: int\n",
    "            Number of elements to randomly sample from the memory in each batch\n",
    "        device: str\n",
    "            Name of the device (cuda or cpu) on which the computations would be performed\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        Tensors representing a batch of transitions sampled from the memory\n",
    "        \"\"\"\n",
    "        if batch_size <= len(self.buffer_state):\n",
    "            indices_to_sample = random.sample(range(len(self.buffer_state)), batch_size)\n",
    "        else:\n",
    "            indices_to_sample = random.choices(range(len(self.buffer_state)), k=batch_size)\n",
    "            \n",
    "\n",
    "        states = torch.from_numpy(np.array(self.buffer_state)[indices_to_sample]).float().to(device)\n",
    "        actions = torch.from_numpy(np.array(self.buffer_action)[indices_to_sample]).to(device)\n",
    "        next_states = torch.from_numpy(np.array(self.buffer_next_state)[indices_to_sample]).float().to(device)\n",
    "        rewards = torch.from_numpy(np.array(self.buffer_reward)[indices_to_sample]).float().to(device)\n",
    "        # a = np.array(self.buffer_reward)[indices_to_sample]\n",
    "        # a=np.vstack(a).astype(np.float)\n",
    "        # rewards = torch.from_numpy(a).float().to(device)\n",
    "        # dones = torch.from_numpy(np.array(self.buffer_done)[indices_to_sample]).to(device)\n",
    "\n",
    "        return states, actions, next_states, rewards\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5181c36e-fbe1-4ec0-a93c-c45421daebd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport random\\nsampled_indices = random.choices(range(10), k=5)\\nsampled_indices\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import random\n",
    "sampled_indices = random.choices(range(10), k=5)\n",
    "sampled_indices\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bcfcf-fedb-446a-ab13-8eb5206c5d41",
   "metadata": {},
   "source": [
    "Returns the size of the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bc40d01-4f0b-443c-953a-e9f75883b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "class ReplayMemory(ReplayMemory):\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function that specifies the number of elements persent in the replay memory\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        none\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        int\n",
    "            number of currently stored elements in the replay buffer\n",
    "        \"\"\"\n",
    "        return len(self.buffer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01c8fe-c10b-4e16-9ff3-fd3cf25c6ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
