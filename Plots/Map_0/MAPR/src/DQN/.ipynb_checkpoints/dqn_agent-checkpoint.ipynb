{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a0707e-db9e-4fc9-99b8-210eee57d8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/DQN/dqn_net.ipynb\n",
    "%run src/DQN/replay_memory.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9820d-21c4-4ab9-bdea-234dcd7de20e",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a7bd5-c918-4fa9-a818-8e9627f839c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script that contains details how the DQN agent learns, updates the target network, selects actions and save/loads the model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f783ab-f268-4a00-b00b-7f07ba9e3542",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eea7017a-85f3-47ae-a554-e2c82ce7c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80bddd-b2af-4cfe-a94c-c9693c97334c",
   "metadata": {},
   "source": [
    "Read the configurations from the signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1dba01a-951a-4052-b900-82fdc9e364c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/config.ini']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from configparser import ConfigParser\n",
    "  \n",
    "configur = ConfigParser()\n",
    "import builtins\n",
    "configur.read(builtins.current_filename)\n",
    "\n",
    "# configur.read('config.ini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc3a868-5c1c-42d6-947b-ce98764f2065",
   "metadata": {},
   "source": [
    "Store the values of the parameters. Don't know what they mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "217cd9e9-6055-4c47-ba20-9951e0bec6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount gamma for the return function.\n",
    "discount1 = float(configur.get('architecture','discount'))\n",
    "# starting value of the epsilon\n",
    "eps_max1= float(configur.get('architecture','eps_max'))\n",
    "# minimum value of the epsilon\n",
    "eps_min1= float(configur.get('architecture','eps_min'))\n",
    "# decay parameter for epsilon\n",
    "eps_decay1 = float(configur.get('architecture','eps_decay'))\n",
    "# Memory capacity of the buffer\n",
    "memory_capacity1 = int(configur.get('architecture','memory_capacity'))\n",
    "# learning rate\n",
    "lr1= float(configur.get('architecture','lr')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b60ec-376f-4dc1-b442-43fd0842bf34",
   "metadata": {},
   "source": [
    "Initialize the device. \\\n",
    "Initialize the epsilon for the epsilon greedy strategy. \\\n",
    "Initialize the discount, gamma for the return function. \\\n",
    "Initialize the state size and action size. \\\n",
    "Initialize the policy network and the target(value) network.\\\n",
    "Initialize the experience buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b588c3d5-3d4d-4474-9d08-25591a106acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    \"\"\"\n",
    "    Class that defines the functions required for training the DQN agent\n",
    "    \"\"\"\n",
    "    def __init__(self, device, state_size, action_size, \n",
    "                    discount=discount1, \n",
    "                    eps_max=eps_max1, \n",
    "                    eps_min=eps_min1, \n",
    "                    eps_decay=eps_decay1, \n",
    "                    memory_capacity=memory_capacity1, \n",
    "                    lr=lr1, \n",
    "                    train_mode=True):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # for epsilon-greedy exploration strategy\n",
    "        self.epsilon = eps_max\n",
    "        self.epsilon_min = eps_min\n",
    "        self.epsilon_decay = eps_decay\n",
    "\n",
    "        # for defining how far-sighted or myopic the agent should be\n",
    "        self.discount = discount\n",
    "\n",
    "        # size of the state vectors and number of possible actions\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        torch_seed = 0\n",
    "        random_seed = 0\n",
    "        cuda_seed = 0\n",
    "        \n",
    "        random.seed(random_seed)\n",
    "        torch.manual_seed(torch_seed)\n",
    "        torch.cuda.manual_seed(cuda_seed)\n",
    "\n",
    "        # instances of the network for current policy and its target\n",
    "        self.policy_net = DQNNet(self.state_size, self.action_size, lr).to(self.device)\n",
    "        self.target_net = DQNNet(self.state_size, self.action_size, lr).to(self.device)\n",
    "        self.target_net.eval() # since no learning is performed on the target net\n",
    "        if not train_mode:\n",
    "            self.policy_net.eval()\n",
    "\n",
    "        # instance of the replay buffer\n",
    "        self.memory = ReplayMemory(capacity=memory_capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16b3b7-e739-458c-b6d7-89623d9061f1",
   "metadata": {},
   "source": [
    "Make the action selection process purely greedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e3b75-f4e3-419f-8fd7-17f02c941529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):    \n",
    "    def turn_off_exploration(self):\n",
    "        self.epsilon =  0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e8024-7096-4e56-b2e7-2370d78a5e52",
   "metadata": {},
   "source": [
    "Update the value or taget DQN with the policy DQN parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70873a79-8e63-4ebc-b4b6-dd86b03062d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):    \n",
    "    def updateTargetNet(self):\n",
    "        \"\"\"\n",
    "        Function to copy the weights of the current policy net into the (frozen) target net\n",
    "\n",
    "        Parameterse\n",
    "        ---\n",
    "        none\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        none\n",
    "        \"\"\"\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb88797-633b-42de-8598-e5a40010c2f7",
   "metadata": {},
   "source": [
    "Update the value of the epsilon function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee8de3-e3b4-465a-a5b9-2c37d972cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):    \n",
    "    def updateEpsilon(self):\n",
    "        \"\"\"\n",
    "        Function for reducing the epsilon value (used for epsilon-greedy exploration with annealing)\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        none\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        none\n",
    "        \"\"\"\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon*self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b0deb7-2c7d-4e3c-a4cf-174cbeefea0e",
   "metadata": {},
   "source": [
    "Select the action in an epsilon greedy manner. \\\n",
    "If the value is less than epsilon, randomly return an integer representing an action. \\\n",
    "Else make the current state as a tensor and feed it to the policy network. \\\n",
    "Choose the argmax of the result of the policy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fad36ed4-cf28-4275-8cf1-ea917f2c1559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):    \n",
    "    def selectAction(self, state):\n",
    "        \"\"\"\n",
    "        Uses epsilon-greedy exploration such that, if the randomly generated number is less than epsilon then the agent performs random action, else the agent executes the action suggested by the policy Q-network\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Function to return the appropriate action for the given state.\n",
    "        During training, returns a randomly sampled action or a greedy action (predicted by the policy network), based on the epsilon value.\n",
    "        During testing, returns action predicted by the policy network\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        state: vector or tensor\n",
    "            The current state of the environment as observed by the agent\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        if random.random() < self.epsilon: # amount of exploration reduces with the epsilon value\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # pick the action with maximum Q-value as per the policy Q-network\n",
    "        with torch.no_grad():\n",
    "            action = self.policy_net.forward(state)\n",
    "        return torch.argmax(action).item() # since actions are discrete, return index that has highest Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f0ae29-9d9a-4f30-9199-8961d88e441b",
   "metadata": {},
   "source": [
    "Sample experience from the replay buffer of size batchsize. \\\n",
    "Use the policy DQN to predict the possible_actions.\\\n",
    "Calculate the actions for each experience selected by the policy DQN. \\\n",
    "Then calculate the rewards of the actions of all the experiences by target or value DQN. Use max on this value which will return the maximum values and indices for each experience and then choose only the values. \\\n",
    "Then discount the Q values of the target DQN by gamma, and add it to the rewards. \\ \n",
    "Make the loss function to minimise the loss between q_pred, and the return y_j. \\\n",
    "Then update the policy network and return the MSE loss. \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b2c258-fcf6-4799-b1c4-0ee60f80f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):    \n",
    "    def learn(self, batchsize):\n",
    "        \"\"\"\n",
    "        Function to perform the updates on the neural network that runs the DQN algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        batchsize: int\n",
    "            Number of experiences to be randomly sampled from the memory for the agent to learn from\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        # select n samples picked uniformly at random from the experience replay memory, such that n=batchsize\n",
    "        if len(self.memory) < batchsize:\n",
    "            return\n",
    "        states, actions, next_states, rewards = self.memory.sample(batchsize, self.device)\n",
    "\n",
    "        # get q values of the actions that were taken, i.e calculate qpred; \n",
    "        # actions vector has to be explicitly reshaped to nx1-vector\n",
    "        q_pred = self.policy_net.forward(states).gather(1, actions.view(-1, 1)) \n",
    "        \n",
    "        #calculate target q-values, such that yj = rj + q(s', a'), but if current state is a terminal state, then yj = rj\n",
    "        q_target = self.target_net.forward(next_states).max(dim=1).values # because max returns data structure with values and indices\n",
    "        #q_target = self.policy_net.forward(next_states).max(dim=1).values # because max returns data structure with values and indices\n",
    "        # q_target[dones] = 0.0 # setting Q(s',a') to 0 when the current state is a terminal state\n",
    "        y_j = rewards + (self.discount * q_target)\n",
    "        y_j = y_j.view(-1, 1)\n",
    "        \n",
    "        # calculate the loss as the mean-squared error of yj and qpred\n",
    "        self.policy_net.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(y_j, q_pred).mean()\n",
    "        loss.backward()\n",
    "        self.policy_net.optimizer.step()\n",
    "        \n",
    "        # TODO: get loss from current values of q_pred and y_j\n",
    "        return float(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ba332-bed2-4bcb-bcfc-3c76e1d9270d",
   "metadata": {},
   "source": [
    "Save the policy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452bd387-7ba5-40a4-b47b-435725892936",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):    \n",
    "    def saveModel(self, filename):\n",
    "        \"\"\"\n",
    "        Function to save the policy network\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        filename: str\n",
    "            Location of the file where the model is to be saved        \n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        self.policy_net.saveModel(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89705f46-6809-4117-b29c-db9685239f04",
   "metadata": {},
   "source": [
    "Load the policy network from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7f0c8-223b-46fd-b4f6-7d1de3e76c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):        \n",
    "    def loadModel(self, filename):\n",
    "        \"\"\"\n",
    "        Function to load model parameters\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        filename: str\n",
    "            Location of the file from where the model is to be loaded\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        none\n",
    "        \"\"\"\n",
    "\n",
    "        self.policy_net.loadModel(filename=filename, device=self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e69b56-a185-47fb-b509-55af002ade65",
   "metadata": {},
   "source": [
    "Make the state in the tensor. \\\n",
    "Obtain the q_values from the target DQN. \\\n",
    "Find the maximum q_value and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8712f5-d21c-41b6-906c-a7606dfa6b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(DQNAgent):        \n",
    "    def getQValue(self, state):\n",
    "        \"\"\"\n",
    "        Function to return the Q-value for the given state(currently returning the MAX)\n",
    "\n",
    "        Parameters\n",
    "        ---\n",
    "        state: vector or tensor\n",
    "            The current state of the environment as observed by the agent\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        q_values: tensor\n",
    "            The Q-values for the given state\n",
    "        \"\"\"\n",
    "\n",
    "        if not torch.is_tensor(state):\n",
    "            state = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # TODO policy_net or target_net?\n",
    "            q_values = self.target_net.forward(state)\n",
    "\n",
    "        max_q_value = float(torch.max(q_values, dim=1)[0])\n",
    "\n",
    "        return max_q_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
